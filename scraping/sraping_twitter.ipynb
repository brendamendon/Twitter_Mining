{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Código de Scraping na busca avançada do Twitter\n",
    "\n",
    "> Desenvolvido por Brenda Mendonça.  \n",
    "> OBS: Parte do código utiliza aplicação de cookies na página de login do Twitter, extraído de um código desenvolvido por [Otávio Iasbeck](https://github.com/OIasbeck/Scrapping_Selenium/tree/main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import pickle\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_driver(driver_type):\n",
    "    # Define qual driver será utilizado\n",
    "    if driver_type == 1:\n",
    "        driver = webdriver.Firefox()\n",
    "    elif driver_type == 2:\n",
    "        driver = webdriver.Chrome()\n",
    "    elif driver_type == 3:\n",
    "        driver = webdriver.Ie()\n",
    "    elif driver_type == 4:\n",
    "        driver = webdriver.Opera()\n",
    "    elif driver_type == 5:\n",
    "        driver = webdriver.PhantomJS()\n",
    "    \n",
    "    # Aguardando 5 segundos para continuar a execução do código\n",
    "    driver.wait = WebDriverWait(driver, 5)\n",
    "\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abre_html(driver, page):\n",
    "    # Abre a página\n",
    "    driver.get(page)\n",
    "    \n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplica_cookies(driver, dir):\n",
    "    try:\n",
    "        # Carrega os cookies\n",
    "        cookies = pickle.load(open(fr'{dir}\\cookies.pkl', \"rb\"))\n",
    "\n",
    "        # Deleta os cookies da página\n",
    "        driver.delete_all_cookies()\n",
    "\n",
    "        for cookie in cookies:\n",
    "            # Verifica a chave 'expiry' do cookie (que é um dicionário)\n",
    "            if isinstance(cookie.get('expiry'), float):\n",
    "                cookie['expiry'] = int(cookie['expiry']) \n",
    "\n",
    "            # Aplica o valor da chave no driver da página\n",
    "            driver.add_cookie(cookie)\n",
    "        print(\"Os cookies foram aplicados com sucesso!\")\n",
    "\n",
    "    except:\n",
    "        print(\"Houve um problema ao aplicar os cookies\")\n",
    "        raise Exception\n",
    "    \n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv(nome, user, tweet_date, tweet_text, emojis):\n",
    "    # Abre o arquivo em modo de escrita\n",
    "    with open(r\"..\\bases\\tweets.csv\", \"a\", encoding='utf-8', newline='') as csv_file:\n",
    "        # Cria o objeto writer\n",
    "        writer = csv.writer(csv_file)\n",
    "\n",
    "        # Grava os dados no arquivo csv como uma nova linha\n",
    "        writer.writerow([nome, user, tweet_date, tweet_text, emojis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tweets(driver, start_date, end_date, word, lang, max_time, diretorio, page):\n",
    "    # Autenticação da página\n",
    "    driver = abre_html(driver, page)\n",
    "    driver = aplica_cookies(driver, dir=diretorio)\n",
    "\n",
    "    # Idiomas para filtrar os tweets\n",
    "    languages = { 1: 'en', 2: 'pt', 3: 'es', 4: 'fr', 5: 'de', 6: 'ru', 7: 'zh'}\n",
    "\n",
    "    # Construção da URL para busca avançada\n",
    "    url = \"https://twitter.com/search?lang=pt&q=\"\n",
    "    url += \"%24{0}%20(%24{0})\".format(word)\n",
    "    url += \"%20lang%3A{}\".format(languages[lang])\n",
    "    url += \"%20until%3A{0}%20since%3A{1}\".format(end_date, start_date)\n",
    "    url += \"%20-filter%3Areplies&src=typed_query\"\n",
    "\n",
    "    # Acessa a página\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Armazena a posição da página antes de rolar\n",
    "    previous_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(8)\n",
    "\n",
    "        # Armazena a posição da página após a rolagem\n",
    "        current_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        # Se as duas posições forem iguais, o loop é interrompido, indicando que a página chegou ao final\n",
    "        if current_height == previous_height:\n",
    "            break\n",
    "        \n",
    "        previous_height = current_height\n",
    "\n",
    "        try:\n",
    "            # Pegando o código html dos tweets\n",
    "            tweet_divs = driver.page_source\n",
    "            soup = BeautifulSoup(tweet_divs, \"html.parser\")\n",
    "            content = soup.find_all('article')\n",
    "\n",
    "            for c in content:\n",
    "                try:\n",
    "                    # Capturando o texto do tweet\n",
    "                    tweet_text = c.find('div', {'data-testid': 'tweetText'}).text\n",
    "                    tweet_text = tweet_text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "                    \n",
    "                    # Capturando o nome e o username da pessoa que fez o tweet\n",
    "                    tweet_name = c.find('div', {'data-testid': 'User-Name'}).text.strip()\n",
    "                    name_parts = tweet_name.split('@')\n",
    "                    nome = name_parts[0]\n",
    "                    nome = nome.replace(',', '')\n",
    "                    user = '@' + name_parts[1].split('·')[0]\n",
    "\n",
    "                    # Capturando a data do tweet\n",
    "                    date_elem = c.find('time')\n",
    "                    tweet_date = date_elem.text.strip()\n",
    "\n",
    "                    # Extrair os emojis do tweet\n",
    "                    emojis = []\n",
    "                    emoji_tags = c.find_all('img')\n",
    "                    for emoji_tag in emoji_tags:\n",
    "                        emoji = emoji_tag.get('alt')\n",
    "                        if emoji and not emoji.startswith('Image'):\n",
    "                            emojis.append(emoji)\n",
    "\n",
    "                    print(f\"Nome: {nome}\")\n",
    "                    print(f\"User: {user}\")\n",
    "                    print(f\"Data: {tweet_date}\")\n",
    "                    print(f\"Tweet: {tweet_text}\")\n",
    "                    print(f\"Emojis: {emojis}\")\n",
    "                    print(\"----------------------\")\n",
    "\n",
    "                    try:\n",
    "                        write_csv(nome, user, tweet_date, tweet_text,emojis)\n",
    "                    except:\n",
    "                        print('csv error')\n",
    "\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Algo de errado aconteceu!\")\n",
    "            print(e)\n",
    "            driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    ''' \n",
    "        CÓDIGO PRINCIPAL \n",
    "    '''\n",
    "\n",
    "    # Chave que define o navegador\n",
    "    driver_type = 2\n",
    "\n",
    "    # Lista de strings a serem buscadas nos tweets\n",
    "    word = 'TSLA'\n",
    "\n",
    "    # Diretório do arquivo .pkl com os cookies de autenticação de login\n",
    "    diretorio = '..\\pkl'\n",
    "\n",
    "    # Pagina inicial para autenticação\n",
    "    page = \"https://twitter.com/home\"\n",
    "\n",
    "    # Define o range de datas para a pesquisa\n",
    "    start_date = '2023-06-10'\n",
    "    end_date = '2023-06-11'\n",
    "\n",
    "    # Chave que define o idioma dos tweets\n",
    "    lang = 1\n",
    "\n",
    "    # Tempo de busca dos tweets em segundos\n",
    "    max_time = 100\n",
    "    \n",
    "    # Inicializa o navegador\n",
    "    driver = init_driver(driver_type)\n",
    "\n",
    "    # Chama a função que faz o scraping passando seus argumentos \n",
    "    scrape_tweets(driver, start_date, end_date, word, lang, max_time, diretorio, page)\n",
    "    time.sleep(5)\n",
    "    print(\"Um arquivo com os tweets foi gerado!\")\n",
    "\n",
    "    # Fecha o navegador e encerra sua instância\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    '''\n",
    "        EXECUÇÃO DO ESCOPO PRINCIPAL\n",
    "    '''\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'https://twitter.com/search?lang=pt&q=%24TSLA%20(%24TSLA)%20lang%3Aen%20until%3A2023-06-28%20since%3A2023-02-28%20-filter%3Areplies&src=typed_query'\n",
    "'https://twitter.com/search?lang=pt&q=%24TSLA%20(%24TSLA)%20lang%3Aen%20until%3A2023-06-28%20since%3A2023-02-28%20-filter%3Areplies&src=typed_query'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
